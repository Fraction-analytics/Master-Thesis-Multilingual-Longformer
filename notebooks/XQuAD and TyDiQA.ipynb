{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets as nlp\n",
    "from transformers import LongformerTokenizerFast\n",
    "from transformers import XLMRobertaTokenizerFast, AutoTokenizer\n",
    "from transformers import XLMRobertaForQuestionAnswering\n",
    "from transformers import RobertaForQuestionAnswering\n",
    "\n",
    "from transformers.utils import logging as hf_logging\n",
    "\n",
    "\n",
    "hf_logging.enable_default_handler()\n",
    "hf_logging.set_verbosity_info()\n",
    "hf_logging.enable_explicit_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|file_utils.py:1127] 2021-01-18 19:37:06,179 >> https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json not found in cache or force_download set to True, downloading to /.cache/torch/transformers/tmp78w531ob\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40b6f5f5f954f33b7645773916fb4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=512.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|file_utils.py:1131] 2021-01-18 19:37:06,628 >> storing https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json in cache at /.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "[INFO|file_utils.py:1134] 2021-01-18 19:37:06,630 >> creating metadata file for /.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "[INFO|configuration_utils.py:395] 2021-01-18 19:37:06,632 >> loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "[INFO|configuration_utils.py:431] 2021-01-18 19:37:06,634 >> Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|file_utils.py:1127] 2021-01-18 19:37:07,040 >> https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model not found in cache or force_download set to True, downloading to /.cache/torch/transformers/tmpm07d7kce\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa94fecb74c4a7ab452fafc01f0e377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|file_utils.py:1131] 2021-01-18 19:37:08,325 >> storing https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model in cache at /.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "[INFO|file_utils.py:1134] 2021-01-18 19:37:08,326 >> creating metadata file for /.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|file_utils.py:1127] 2021-01-18 19:37:08,742 >> https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-tokenizer.json not found in cache or force_download set to True, downloading to /.cache/torch/transformers/tmp859p4i0q\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223bc3e1343645a6838ec577600a548c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=9096718.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|file_utils.py:1131] 2021-01-18 19:37:11,968 >> storing https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-tokenizer.json in cache at /.cache/torch/transformers/2548f9b79ad6e35303efa08cc1d173b32a2727a8314ca7283ed4f61b3608721f.dd9d4117ae6562a3d1d4d19d203f216c41ba7968fc47c83bbd199df99dca4871\n",
      "[INFO|file_utils.py:1134] 2021-01-18 19:37:11,971 >> creating metadata file for /.cache/torch/transformers/2548f9b79ad6e35303efa08cc1d173b32a2727a8314ca7283ed4f61b3608721f.dd9d4117ae6562a3d1d4d19d203f216c41ba7968fc47c83bbd199df99dca4871\n",
      "[INFO|tokenization_utils_base.py:1599] 2021-01-18 19:37:11,972 >> loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "[INFO|tokenization_utils_base.py:1599] 2021-01-18 19:37:11,973 >> loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-tokenizer.json from cache at /.cache/torch/transformers/2548f9b79ad6e35303efa08cc1d173b32a2727a8314ca7283ed4f61b3608721f.dd9d4117ae6562a3d1d4d19d203f216c41ba7968fc47c83bbd199df99dca4871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base', use_fast=True)\n",
    "#tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base', use_fast=True)\n",
    "#tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong version, but used for illustration\n",
    "#tok = XLMRobertaTokenizerFast.from_pretrained('/workspace/models/xlm-roberta-base-4096-seed-42-lm', use_data=True)\n",
    "#tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Needs retraining!\\n!ls /workspace/models/xlm-roberta-base-4096-seed-42-lm\\n\\n# UGLY TEMPARARY FIX!\\n!cp /workspace/models/xlm-roberta-base-4096-seed-42-lm-TEST/vocab.json /workspace/models/xlm-roberta-base-4096-seed-42-lm\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Needs retraining!\n",
    "!ls /workspace/models/xlm-roberta-base-4096-seed-42-lm\n",
    "\n",
    "# UGLY TEMPARARY FIX!\n",
    "!cp /workspace/models/xlm-roberta-base-4096-seed-42-lm-TEST/vocab.json /workspace/models/xlm-roberta-base-4096-seed-42-lm\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check version\n",
    "# https://github.com/huggingface/transformers/pull/3198"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e6c891e7914da7b9c28e4ca4d33f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2009.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600f937d8e6e491d8c385772785ced32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2305.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset xquad/xquad.en (download: 12.68 MiB, generated: 1.07 MiB, post-processed: Unknown size, total: 13.75 MiB) to /.cache/huggingface/datasets/xquad/xquad.en/1.0.0/9e12114a409c05777407a840606169d55d5ffb5ca8003000da5325a25fd55cd3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23da0ca4695f4ac18e9058f524b07d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=247072.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a871f5666a0248d88bfa6c7ebf900234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=156720.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab44b77aa054adba082fcd89241f496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=164348.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41749a9676664d0e854ef0b665c6b015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=177731.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad795a10251d407b8db422ae348a875e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=136345.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666f6348a70a4ce692ab921f5914c302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=152200.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e306c62d04aa49e08aa1d1a7296651c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=265357.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a423d4f028e474bada644b9e45ca0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=308170.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6d1f44a3924957b8a6fe605c48f016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=275405.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f76653ca7b540b289883e2a79e6c539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=157348.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e7b4f8a09d4d8fbf1aacb24abb9eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=297977.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0b54349f97496b81e8cf31ecbfd6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset xquad downloaded and prepared to /.cache/huggingface/datasets/xquad/xquad.en/1.0.0/9e12114a409c05777407a840606169d55d5ffb5ca8003000da5325a25fd55cd3. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset xquad/xquad.ru (download: 12.68 MiB, generated: 2.04 MiB, post-processed: Unknown size, total: 14.72 MiB) to /.cache/huggingface/datasets/xquad/xquad.ru/1.0.0/9e12114a409c05777407a840606169d55d5ffb5ca8003000da5325a25fd55cd3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5152f9d4c9ab42c59dce6017ece04dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset xquad downloaded and prepared to /.cache/huggingface/datasets/xquad/xquad.ru/1.0.0/9e12114a409c05777407a840606169d55d5ffb5ca8003000da5325a25fd55cd3. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset xquad/xquad.ar (download: 12.68 MiB, generated: 1.64 MiB, post-processed: Unknown size, total: 14.33 MiB) to /.cache/huggingface/datasets/xquad/xquad.ar/1.0.0/9e12114a409c05777407a840606169d55d5ffb5ca8003000da5325a25fd55cd3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0934af1fe57f48b4a871b86ad2b81123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset xquad downloaded and prepared to /.cache/huggingface/datasets/xquad/xquad.ar/1.0.0/9e12114a409c05777407a840606169d55d5ffb5ca8003000da5325a25fd55cd3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'context', 'question', 'answers'],\n",
       "    num_rows: 1190\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "English\n",
    "Spanish, German, Greek, \n",
    "Russian, Turkish, Arabic, \n",
    "Vietnamese, Thai, Chinese, \n",
    "and Hindi.\n",
    "\"\"\"\n",
    "# Each of these are validation datasets\n",
    "xquad_en = nlp.load_dataset('xquad', 'xquad.en', split=\"validation\")\n",
    "xquad_ru = nlp.load_dataset('xquad', 'xquad.ru', split=\"validation\")\n",
    "xquad_ar = nlp.load_dataset('xquad', 'xquad.ar', split=\"validation\")\n",
    "xquad_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTyDiQA\\n\\nEnglish 3696 141.8 125 641 147.5 132\\nArabic 14805 192.1 150 1025 166.7 135\\nBengali 2390 277.6 230 130 283.3 246\\nFinnish 6855 141.0 120 1082 147.9 125\\nIndonesian 5702 135.0 114 746 131.7 109\\nSwahili 2755 115.3 75 671 98.1 76\\nKorean 1625 191.0 142 441 161.8 138\\nRussian 6490 176.8 136 913 163.3 133\\nTelugu \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TyDiQA\n",
    "\n",
    "English 3696 141.8 125 641 147.5 132\n",
    "Arabic 14805 192.1 150 1025 166.7 135\n",
    "Bengali 2390 277.6 230 130 283.3 246\n",
    "Finnish 6855 141.0 120 1082 147.9 125\n",
    "Indonesian 5702 135.0 114 746 131.7 109\n",
    "Swahili 2755 115.3 75 671 98.1 76\n",
    "Korean 1625 191.0 142 441 161.8 138\n",
    "Russian 6490 176.8 136 913 163.3 133\n",
    "Telugu \n",
    "\"\"\"\n",
    "\n",
    "#english\n",
    "#arabic\n",
    "#russian\n",
    "\n",
    "# tydi_train, tydi_valid = nlp.load_dataset('tydiqa', 'secondary_task', split=['train', 'validation']) # has option: ['primary_task', 'secondary_task'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_alignement(context: str, answer):\n",
    "    \"\"\" Some original examples in SQuAD have indices wrong by 1 or 2 character. We test and fix this here. \"\"\"\n",
    "    gold_text = answer[\"text\"][0]\n",
    "    start_idx = answer[\"answer_start\"][0]\n",
    "    end_idx = start_idx + len(gold_text)\n",
    "    if context[start_idx:end_idx] == gold_text:\n",
    "        return start_idx, end_idx  # When the gold label position is good\n",
    "    elif context[start_idx - 1 : end_idx - 1] == gold_text:\n",
    "        return start_idx - 1, end_idx - 1  # When the gold label is off by one character\n",
    "    elif context[start_idx - 2 : end_idx - 2] == gold_text:\n",
    "        return start_idx - 2, end_idx - 2  # When the gold label is off by two character\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "\n",
    "# Tokenize our training dataset\n",
    "def convert_to_features(example):\n",
    "    # Tokenize contexts and questions (as pairs of inputs)\n",
    "    encodings = tokenizer.encode_plus(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        pad_to_max_length=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "    )\n",
    "    context_encodings = tokenizer.encode_plus(example[\"context\"])\n",
    "\n",
    "    # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methodes.\n",
    "    # this will give us the position of answer span in the context text\n",
    "    start_idx, end_idx = get_correct_alignement(example[\"context\"], example[\"answers\"])\n",
    "    start_positions_context = context_encodings.char_to_token(start_idx)\n",
    "    end_positions_context = context_encodings.char_to_token(end_idx - 1)\n",
    "    \n",
    "    # FIXME: UGLY HACK because of XLM-R tokenization, works fine with monolingual\n",
    "    # 2 training examples returns incorrect positions\n",
    "    sep_idx = encodings[\"input_ids\"].index(tokenizer.sep_token_id)\n",
    "    try:\n",
    "        # here we will compute the start and end position of the answer in the whole example\n",
    "        # as the example is encoded like this <s> question</s></s> context</s>\n",
    "        # and we know the postion of the answer in the context\n",
    "        # we can just find out the index of the sep token and then add that to position + 1 (+1 because there are two sep tokens)\n",
    "        # this will give us the position of the answer span in whole example\n",
    "        \n",
    "        start_positions = start_positions_context + sep_idx + 1\n",
    "        end_positions = end_positions_context + sep_idx + 1\n",
    "\n",
    "        if end_positions > 512:\n",
    "            start_positions, end_positions = 0, 0\n",
    "    \n",
    "    # Returned None for start or end position index\n",
    "    except:\n",
    "        #print(f\"{example}\")\n",
    "        #print(f\"Start_idx: {start_idx} \\t End_idx: {end_idx}\")\n",
    "        #print(f\"Sep_idx: {sep_idx}\")\n",
    "        #print(f\"with start: {start_positions_context} \\t end: {end_positions_context}\\n\")\n",
    "        start_positions = None\n",
    "        end_positions = None\n",
    "    \n",
    "    encodings.update(\n",
    "        {\n",
    "            \"start_positions\": start_positions,\n",
    "            \"end_positions\": end_positions,\n",
    "            \"attention_mask\": encodings[\"attention_mask\"],\n",
    "        }\n",
    "    )\n",
    "    return encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a5cdc08dd24c7e9825ea1e5992d355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2043.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376cef7c006546f4a471d7fb5d10efda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=986.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.75 MiB, post-processed: Unknown size, total: 119.27 MiB) to /.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bc90b15908495eac8d708bfe4dcb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=8116577.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a86fa451924a618c43fa266a4f74ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1054280.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb676a47271435fb47377bdf9e1afa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fa51ea26164edfa1d5261ee7c240a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad downloaded and prepared to /.cache/huggingface/datasets/squad/plain_text/1.0.0/1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92c2f13e021474cace92852dc63f3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=87599.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebf64c244e5464facfafd5569eb6a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=88.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915d76bc0d67410496e4a93c7e947f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae8c0472cea42cd993d97bbc1382f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: \\nFutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, \\nuse `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, \\nor use `padding='max_length'` to pad to a max length. \\nIn this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or \\nleave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\\n  FutureWarning,\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When using only SQuAD\n",
    "\n",
    "\"\"\"\n",
    "## load train and validation split of squad\n",
    "train_dataset  = nlp.load_dataset('squad', split=nlp.Split.TRAIN)\n",
    "valid_dataset = nlp.load_dataset('squad', split=nlp.Split.VALIDATION)\n",
    "print(\"Unchanged: ==>\", len(valid_dataset), len(xquad_en))\n",
    "#valid_dataset = xquad_en\n",
    "\n",
    "\n",
    "# Add \n",
    "train_dataset = train_dataset.map(convert_to_features).filter(lambda example: (example['start_positions'] is not None) and (example['end_positions'] is not None))\n",
    "valid_dataset = valid_dataset.map(convert_to_features).filter(lambda example: (example['start_positions'] is not None) and (example['end_positions'] is not None))\n",
    "\n",
    "# set the tensor type and the columns which the dataset should return\n",
    "columns = ['input_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "valid_dataset.set_format(type='torch', columns=columns)\n",
    "\n",
    "\n",
    "#print(len(train_dataset), len(squad_train))\n",
    "print(\"Modded: ==>\",len(valid_dataset), len(xquad_en))\n",
    "\"\"\"\n",
    "\n",
    "# When using XQuAD\n",
    "# train dataset\n",
    "squad_train, squad_valid = nlp.load_dataset('squad', split=['train', 'validation'])\n",
    "train_dataset = squad_train.map(convert_to_features).filter(lambda example: (example['start_positions'] is not None) and (example['end_positions'] is not None))\n",
    "valid_dataset = squad_valid.map(convert_to_features).filter(lambda example: (example['start_positions'] is not None) and (example['end_positions'] is not None))\n",
    "\n",
    "\n",
    "# set the tensor type and the columns which the dataset should return\n",
    "columns = ['input_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "train_dataset.set_format(type='torch', columns=columns)\n",
    "valid_dataset.set_format(type='torch', columns=columns)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: \n",
    "FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, \n",
    "use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, \n",
    "or use `padding='max_length'` to pad to a max length. \n",
    "In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or \n",
    "leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
    "  FutureWarning,\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 87599\n",
      "}) Dataset({\n",
      "    features: ['answers', 'attention_mask', 'context', 'end_positions', 'id', 'input_ids', 'question', 'start_positions', 'title'],\n",
      "    num_rows: 87557\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# train: 87599 \n",
    "# val:   10570\n",
    "# \n",
    "# xquad:  1190\n",
    "print(squad_train, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 10570\n",
      "}) Dataset({\n",
      "    features: ['answers', 'attention_mask', 'context', 'end_positions', 'id', 'input_ids', 'question', 'start_positions', 'title'],\n",
      "    num_rows: 10551\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(squad_valid, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cach the dataset, so we can load it directly for training\n",
    "\n",
    "torch.save(train_dataset, 'train_data.pt')\n",
    "torch.save(valid_dataset, 'valid_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - Change padding to padding=True\n",
    "# - Set a max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    #'xlm-roberta-base', \n",
    "    \"n_gpu\": 1,\n",
    "    \"model_name_or_path\": 'xlm-roberta-base',\n",
    "    \"max_len\": 512 ,\n",
    "    \"output_dir\": './models',\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"do_train\": True, \n",
    "    \"do_eval\": True,\n",
    "    \"max_steps\": 100,\n",
    "    \"logging_steps\": 50,\n",
    "    \"eval_steps\": 50,\n",
    "    \"prediction_loss_only\": True, \n",
    "    \"seed\": 42,\n",
    "    \"max_seq_length\": 384, \n",
    "    \"doc_stride\": 128, \n",
    "    \"evaluate_during_training\": True, \n",
    "    #\"evaluation_strategy\": \"steps\",\n",
    "    \"fp16\": True,\n",
    "    \"do_lower_case\": True, \n",
    "}\n",
    "\n",
    "with open('args.json', 'w') as f:\n",
    "    json.dump(args_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import LongformerForQuestionAnswering, LongformerTokenizerFast, EvalPrediction\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    DataCollator,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# @dataclass\n",
    "class DummyDataCollator:\n",
    "    def __call__(self, batch: List) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Take a list of samples from a Dataset and collate them into a batch.\n",
    "        Returns:\n",
    "            A dictionary of tensors\n",
    "        \"\"\"\n",
    "        input_ids = torch.stack([example['input_ids'] for example in batch])\n",
    "        attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
    "        start_positions = torch.stack([example['start_positions'] for example in batch])\n",
    "        end_positions = torch.stack([example['end_positions'] for example in batch])\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'start_positions': start_positions, \n",
    "            'end_positions': end_positions,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "    train_file_path: Optional[str] = field(\n",
    "        default='train_data.pt',\n",
    "        metadata={\"help\": \"Path for cached train dataset\"},\n",
    "    )\n",
    "    valid_file_path: Optional[str] = field(\n",
    "        default='valid_data.pt',\n",
    "        metadata={\"help\": \"Path for cached valid dataset\"},\n",
    "    )\n",
    "    max_len: Optional[int] = field(\n",
    "        default=512,\n",
    "        metadata={\"help\": \"Max input length for the source text\"},\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "\n",
    "    # we will load the arguments from a json file, \n",
    "    # make sure you save the arguments in at ./args.json\n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath('args.json'))\n",
    "\n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        training_args.local_rank,\n",
    "        training_args.device,\n",
    "        training_args.n_gpu,\n",
    "        bool(training_args.local_rank != -1),\n",
    "        training_args.fp16,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=True,\n",
    "    )\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "\n",
    "    # Get datasets\n",
    "    print('loading data')\n",
    "    train_dataset  = torch.load(data_args.train_file_path)\n",
    "    valid_dataset = torch.load(data_args.valid_file_path)\n",
    "    print('loading done')\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        data_collator=DummyDataCollator(),\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        trainer.train(\n",
    "            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "        )\n",
    "        trainer.save_model()\n",
    "        # For convenience, we also re-save the tokenizer to the same directory,\n",
    "        # so that you can share your model easily on huggingface.co/models =)\n",
    "        if trainer.is_world_process_zero():\n",
    "            tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if training_args.do_eval and training_args.local_rank in [-1, 0]:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        eval_output = trainer.evaluate()\n",
    "        print(eval_output)\n",
    "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(eval_output.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(eval_output[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(eval_output[key])))\n",
    "    \n",
    "        results.update(eval_output)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def _mp_fn(index):\n",
    "    # For xla_spawn (TPUs)\n",
    "    main()\n",
    "    \n",
    "    args.prediction_loss_only=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/transformers/training_args.py:339: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
      "  FutureWarning,\n",
      "[INFO|training_args.py:386] 2021-01-18 19:42:20,568 >> PyTorch: setting up devices\n",
      "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "[INFO|configuration_utils.py:395] 2021-01-18 19:42:20,992 >> loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "[INFO|configuration_utils.py:431] 2021-01-18 19:42:20,995 >> Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1599] 2021-01-18 19:42:21,872 >> loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model from cache at /.cache/torch/transformers/0c370616ddfc06067c0634160f749c2cf9d8da2c50e03a2617ce5841c8df3b1d.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed\n",
      "[INFO|tokenization_utils_base.py:1599] 2021-01-18 19:42:21,874 >> loading file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-tokenizer.json from cache at /.cache/torch/transformers/2548f9b79ad6e35303efa08cc1d173b32a2727a8314ca7283ed4f61b3608721f.dd9d4117ae6562a3d1d4d19d203f216c41ba7968fc47c83bbd199df99dca4871\n",
      "[INFO|configuration_utils.py:395] 2021-01-18 19:42:22,941 >> loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-config.json from cache at /.cache/torch/transformers/762ddd751172e9d3229e5da17a459eee6c0dfdc237c718944d0b1a85f06c7e1e.2b0f807393c56e8861a31cd67d2fc0b45d71d9735dd47dd66afb650f90b6d2a8\n",
      "[INFO|configuration_utils.py:431] 2021-01-18 19:42:22,943 >> Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "[INFO|file_utils.py:1127] 2021-01-18 19:42:23,100 >> https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /.cache/torch/transformers/tmp9c1_ocef\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bbc6f083f6408ebcad5f680d5aacc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1115590446.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|file_utils.py:1131] 2021-01-18 19:43:07,363 >> storing https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin in cache at /.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267\n",
      "[INFO|file_utils.py:1134] 2021-01-18 19:43:07,364 >> creating metadata file for /.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267\n",
      "[INFO|modeling_utils.py:940] 2021-01-18 19:43:07,365 >> loading weights file https://cdn.huggingface.co/xlm-roberta-base-pytorch_model.bin from cache at /.cache/torch/transformers/5cbeb972feded79b927818648bf14dc71b7810cda88c8c971a9d45c0dab901ec.aeeaca90954dc20ffa2909de722cfbfd455c5bb16d480c5bdf6d7fe79c68c267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|modeling_utils.py:1047] 2021-01-18 19:43:16,965 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:1058] 2021-01-18 19:43:16,966 >> Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "loading done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:289] 2021-01-18 19:43:19,216 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:370] 2021-01-18 19:43:19,217 >> The following columns in the training set don't have a corresponding argument in `XLMRobertaForQuestionAnswering.forward` and have been ignored: answers, id, question, title, context.\n",
      "[INFO|trainer.py:370] 2021-01-18 19:43:19,218 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForQuestionAnswering.forward` and have been ignored: answers, id, question, title, context.\n",
      "[INFO|trainer.py:677] 2021-01-18 19:43:19,278 >> ***** Running training *****\n",
      "[INFO|trainer.py:678] 2021-01-18 19:43:19,278 >>   Num examples = 87557\n",
      "[INFO|trainer.py:679] 2021-01-18 19:43:19,280 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:680] 2021-01-18 19:43:19,280 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:681] 2021-01-18 19:43:19,281 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:682] 2021-01-18 19:43:19,281 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:683] 2021-01-18 19:43:19,282 >>   Total optimization steps = 100\n",
      "/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py:850: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='51' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 51/100 00:25 < 00:25, 1.91 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='10551' max='10551' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10551/10551 05:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1319] 2021-01-18 19:43:45,568 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1320] 2021-01-18 19:43:45,569 >>   Num examples = 10551\n",
      "[INFO|trainer.py:1321] 2021-01-18 19:43:45,570 >>   Batch size = 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'eval_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f9ef173b5f48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0musr\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpython3\u001b[0m\u001b[0;36m.6\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m347\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mevaluate_during_training\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0margument\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdeprecated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfavor\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mevaluation_strategy\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwhich\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mmore\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-88f23366c875>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         trainer.train(\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    156\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    784\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evalute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_epoch_stop\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evalute\u001b[0;34m(self, tr_loss, model, trial, epoch)\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_evaluate\u001b[0;34m(self, args, state, control, metrics)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_evaluate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0meval_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m             \u001b[0;31m# A Callback can skip the return of `control` if it doesn't change it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/transformers/utils/notebook.py\u001b[0m in \u001b[0;36mon_evaluate\u001b[0;34m(self, args, state, control, metrics, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Step\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Validation Loss\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m             \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total_flos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'eval_loss'"
     ]
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "\"\"\"\n",
    "/usr/local/lib/python3.6/dist-packages/transformers/training_args.py:347: \n",
    "FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
    "  FutureWarning,\n",
    "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
    "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1115590446.0, style=ProgressStyle(descr…\n",
    "\n",
    "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing \n",
    "XLMRobertaForQuestionAnswering: \n",
    "['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
    "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "loading data\n",
    "loading done\n",
    "\n",
    "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:281: \n",
    "FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. \n",
    "Use `args.prediction_loss_only` instead. Setting `args.prediction_loss_only=True\n",
    "  FutureWarning,\n",
    "  \n",
    "/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py:850: \n",
    "UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. \n",
    "This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. \n",
    "You may want to copy the array to protect its data or make it writeable before converting it to a tensor. \n",
    "This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
    "\n",
    "  return torch.tensor(x, **format_kwargs)\n",
    "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: \n",
    "Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, y\n",
    "ou should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  \n",
    "Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. \n",
    "See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SQuAD evaluation script. Modifed slightly for this notebook\n",
    "\n",
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluate(gold_answers, predictions):\n",
    "    f1 = exact_match = total = 0\n",
    "\n",
    "    for ground_truths, prediction in zip(gold_answers, predictions):\n",
    "        total += 1\n",
    "        exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(\n",
    "            f1_score, prediction, ground_truths)\n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LongformerTokenizerFast, LongformerForQuestionAnswering\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained('models')\n",
    "model = LongformerForQuestionAnswering.from_pretrained('models')\n",
    "model = model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = torch.load('valid_data.pt')\n",
    "dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        start_scores, end_scores = model(input_ids=batch['input_ids'].cuda(),\n",
    "                                      attention_mask=batch['attention_mask'].cuda())\n",
    "        for i in range(start_scores.shape[0]):\n",
    "            all_tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][i])\n",
    "            answer = ' '.join(all_tokens[torch.argmax(start_scores[i]) : torch.argmax(end_scores[i])+1])\n",
    "            ans_ids = tokenizer.convert_tokens_to_ids(answer.split())\n",
    "            answer = tokenizer.decode(ans_ids)\n",
    "            answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "valid_dataset.reset_format()\n",
    "for ref, pred in zip(valid_dataset, answers):\n",
    "    predictions.append(pred)\n",
    "    references.append(ref['answers']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaTokenizerFast, AutoTokenizer\n",
    "tok = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', use_fast=True)\n",
    "tok_fast = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
    "tok_auto = AutoTokenizer.from_pretrained('xlm-roberta-base', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d95b1944b184bddbdb7bbd5f6b89bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3721.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0097ad23f99044babeb1988a5766f7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1580.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Config name is missing.\nPlease pick one among the available configs: ['rc', 'rc.nocontext', 'unfiltered', 'unfiltered.nocontext']\nExample of usage:\n\t`load_dataset('trivia_qa', 'rc')`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1726ab02ed4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrivia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trivia_qa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mhash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m     )\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writer_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"writer_batch_size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writer_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cache_dir, name, hash, features, **config_kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mcustom_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         )\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_create_builder_config\u001b[0;34m(self, name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;34m\"Config name is missing.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;34m\"\\nPlease pick one among the available configs: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\nExample of usage:\\n\\t`{}`\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_of_usage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 )\n\u001b[1;32m    225\u001b[0m             \u001b[0mbuilder_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBUILDER_CONFIGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Config name is missing.\nPlease pick one among the available configs: ['rc', 'rc.nocontext', 'unfiltered', 'unfiltered.nocontext']\nExample of usage:\n\t`load_dataset('trivia_qa', 'rc')`"
     ]
    }
   ],
   "source": [
    "trivia = nlp.load_dataset(\"trivia_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, \n",
    "    use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, \n",
    "    or use `padding='max_length'` to pad to a max length. \n",
    "    In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) \n",
    "    or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
