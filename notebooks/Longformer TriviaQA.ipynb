{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nlp\n",
    "import torch\n",
    "import datasets\n",
    "\n",
    "# ATTENTION. Rerunning this command remove the cached trivia qa dataset completely \n",
    "#!rm -rf /.cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivia_qa  wikitext-103-raw\n",
      "mkdir: cannot create directory '../data/trivia_qa': File exists\n"
     ]
    }
   ],
   "source": [
    "# https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb#scrollTo=wyDYG4YDXFV7\n",
    "!ls ../data\n",
    "!mkdir ../data/trivia_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "validation_dataset = datasets.load_dataset(\"trivia_qa\", \"rc\", split=\"validation[:5%]\", cache_dir=\"/workspace/data/trivia_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.11.0\n",
      "apex==0.1\n",
      "argon2-cffi==20.1.0\n",
      "asn1crypto==0.24.0\n",
      "async-generator==1.10\n",
      "attrs==20.3.0\n",
      "backcall==0.2.0\n",
      "bleach==3.2.1\n",
      "cached-property==1.5.2\n",
      "cachetools==4.1.1\n",
      "certifi==2020.11.8\n",
      "cffi==1.14.4\n",
      "chardet==3.0.4\n",
      "click==7.1.2\n",
      "cloudpickle==1.6.0\n",
      "colorama==0.4.4\n",
      "contextvars==2.4\n",
      "cryptography==2.1.4\n",
      "cycler==0.10.0\n",
      "Cython==0.29.21\n",
      "dask==2.30.0\n",
      "dataclasses==0.8\n",
      "datasets==1.1.3\n",
      "decorator==4.4.2\n",
      "defusedxml==0.6.0\n",
      "dill==0.3.3\n",
      "distributed==2.30.1\n",
      "dnspython==2.0.0\n",
      "docopt==0.6.2\n",
      "entrypoints==0.3\n",
      "filelock==3.0.12\n",
      "future==0.18.2\n",
      "gitdb==4.0.5\n",
      "GitPython==3.1.11\n",
      "google-auth==1.23.0\n",
      "google-auth-oauthlib==0.4.2\n",
      "graphviz==0.15\n",
      "grpcio==1.33.2\n",
      "h5py==3.1.0\n",
      "HeapDict==1.0.1\n",
      "hiddenlayer==0.3\n",
      "idna==2.6\n",
      "immutables==0.14\n",
      "importlib-metadata==3.1.0\n",
      "intel-openmp==2020.0.133\n",
      "ipykernel==5.3.4\n",
      "ipython==7.16.1\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.5.1\n",
      "jedi==0.17.2\n",
      "Jinja2==2.11.2\n",
      "joblib==0.17.0\n",
      "json5==0.9.5\n",
      "jsonpickle==1.4.1\n",
      "jsonschema==3.2.0\n",
      "jupyter-client==6.1.7\n",
      "jupyter-core==4.7.0\n",
      "jupyterlab==2.2.9\n",
      "jupyterlab-pygments==0.1.2\n",
      "jupyterlab-server==1.2.0\n",
      "keyring==10.6.0\n",
      "keyrings.alt==3.0\n",
      "kiwisolver==1.3.1\n",
      "Markdown==3.3.3\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.3.3\n",
      "mistune==0.8.4\n",
      "mkl==2019.0\n",
      "msgpack==1.0.0\n",
      "multiprocess==0.70.11.1\n",
      "munch==2.5.0\n",
      "nbclient==0.5.1\n",
      "nbconvert==6.0.7\n",
      "nbformat==5.0.8\n",
      "nest-asyncio==1.4.3\n",
      "notebook==6.1.5\n",
      "numpy==1.19.4\n",
      "oauthlib==3.1.0\n",
      "packaging==20.4\n",
      "pandas==1.1.4\n",
      "pandocfilters==1.4.3\n",
      "parso==0.7.1\n",
      "pexpect==4.8.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==8.0.1\n",
      "prometheus-client==0.9.0\n",
      "prompt-toolkit==3.0.8\n",
      "protobuf==3.14.0\n",
      "psutil==5.7.3\n",
      "ptyprocess==0.6.0\n",
      "py-cpuinfo==7.0.0\n",
      "pyarrow==2.0.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycparser==2.20\n",
      "pycrypto==2.6.1\n",
      "Pygments==2.7.2\n",
      "pygobject==3.26.1\n",
      "pymongo==3.11.1\n",
      "pyparsing==2.4.7\n",
      "pyrsistent==0.17.3\n",
      "python-dateutil==2.8.1\n",
      "pytz==2020.4\n",
      "pyxdg==0.25\n",
      "PyYAML==5.3.1\n",
      "pyzmq==20.0.0\n",
      "regex==2020.11.13\n",
      "requests==2.25.0\n",
      "requests-oauthlib==1.3.0\n",
      "rsa==4.6\n",
      "sacred==0.8.1\n",
      "sacremoses==0.0.43\n",
      "scikit-learn==0.23.2\n",
      "scipy==1.5.4\n",
      "seaborn==0.11.0\n",
      "SecretStorage==2.3.1\n",
      "Send2Trash==1.5.0\n",
      "sentencepiece==0.1.94\n",
      "six==1.11.0\n",
      "sklearn==0.0\n",
      "smmap==3.0.4\n",
      "sortedcontainers==2.3.0\n",
      "tblib==1.7.0\n",
      "tensorboard==2.4.0\n",
      "tensorboard-plugin-wit==1.7.0\n",
      "terminado==0.9.1\n",
      "testpath==0.4.4\n",
      "threadpoolctl==2.1.0\n",
      "tokenizers==0.9.2\n",
      "toolz==0.11.1\n",
      "torch==1.7.0\n",
      "torchsummary==1.5.1\n",
      "tornado==6.1\n",
      "tqdm==4.49.0\n",
      "traitlets==4.3.3\n",
      "transformers==3.4.0\n",
      "typing-extensions==3.7.4.3\n",
      "urllib3==1.26.2\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "Werkzeug==1.0.1\n",
      "wget==3.2\n",
      "widgetsnbextension==3.5.1\n",
      "wrapt==1.12.1\n",
      "xxhash==2.0.0\n",
      "zict==2.0.0\n",
      "zipp==3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the mapping function\n",
    "def format_dataset(example):\n",
    "    # the context might be comprised of multiple contexts => me merge them here\n",
    "    example[\"context\"] = \" \".join((\"\\n\".join(example[\"entity_pages\"][\"wiki_context\"])).split(\"\\n\"))\n",
    "    example[\"targets\"] = example[\"answer\"][\"aliases\"]\n",
    "    example[\"norm_target\"] = example[\"answer\"][\"normalized_value\"]\n",
    "    return example\n",
    "\n",
    "# map the dataset and throw out all unnecessary columns\n",
    "validation_dataset = validation_dataset.map(format_dataset, remove_columns=[\"search_results\", \"question_source\", \"entity_pages\", \"answer\", \"question_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = validation_dataset.filter(lambda x: len(x[\"context\"]) > 0)\n",
    "# check out how many samples are left\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nLength for each example\")\n",
    "print(30 * \"=\")\n",
    "\n",
    "# length for each example\n",
    "validation_dataset.map(lambda x, i: print(f\"Id: {i} - Question Length: {len(x['question'])} - context Length: {len(x['context'])}\"), with_indices=True)\n",
    "print(30 * \"=\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Num examples larger than 4 * 4096 characters: \")\n",
    "# filter out examples smaller than 4 * 4096\n",
    "short_validation_dataset = validation_dataset.filter(lambda x: (len(x['question']) + len(x['context'])) < 4 * 4096)\n",
    "short_validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizerFast, LongformerForQuestionAnswering\n",
    "\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(\"allenai/longformer-large-4096-finetuned-triviaqa\")\n",
    "\n",
    "# download the 1.7 GB pretrained model. It might take ~1min\n",
    "model = LongformerForQuestionAnswering.from_pretrained(\"allenai/longformer-large-4096-finetuned-triviaqa\")\n",
    "model.to(\"cuda\")\n",
    "\n",
    "def evaluate(example):\n",
    "    def get_answer(question, context):\n",
    "        # encode question and context so that they are seperated by a tokenizer.sep_token and cut at max_length\n",
    "        encoding = tokenizer.encode_plus(question, context, return_tensors=\"pt\", max_length=4096, truncation=True)\n",
    "        input_ids = encoding[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = encoding[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "        # the forward method will automatically set global attention on question tokens\n",
    "        # The scores for the possible start token and end token of the answer are retrived\n",
    "        # wrap the function in torch.no_grad() to save memory\n",
    "        with torch.no_grad():\n",
    "            start_scores, end_scores = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Let's take the most likely token using `argmax` and retrieve the answer\n",
    "        all_tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0].tolist())\n",
    "        answer_tokens = all_tokens[torch.argmax(start_scores): torch.argmax(end_scores)+1]\n",
    "        answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))[1:].replace('\"', '')  # remove space prepending space token and remove unnecessary '\"'\n",
    "        \n",
    "        return answer\n",
    "\n",
    "    # save the model's outut here\n",
    "    example[\"output\"] = get_answer(example[\"question\"], example[\"context\"])\n",
    "\n",
    "    # save if it's a match or not\n",
    "    example[\"match\"] = (example[\"output\"] in example[\"targets\"]) or (example[\"output\"] == example[\"norm_target\"])\n",
    "\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_short = short_validation_dataset.map(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nNum Correct examples: {sum(results_short['match'])}/{len(results_short)}\")\n",
    "wrong_results = results_short.filter(lambda x: x['match'] is False)\n",
    "print(f\"\\nWrong examples: \")\n",
    "wrong_results.map(lambda x, i: print(f\"{i} - Output: {x['output']} - Target: {x['norm_target']}\"), with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = validation_dataset.map(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correct examples: {sum(results['match'])}/{len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TriviaQA json to SQUAD format dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def read_squad_files(path: str):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers\n",
    "    \n",
    "\n",
    "train_contexts, train_questions, train_answers = read_squad_files('/workspace/data/trivia_squad/squad-wikipedia-train-4096.json')\n",
    "val_contexts, val_questions, val_answers = read_squad_files('/workspace/data/trivia_squad/squad-wikipedia-dev-4096.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add start and end tokens correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two – fix this\n",
    "        if context[start_idx:end_idx].lower() == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1].lower() == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2].lower() == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(val_answers, val_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', do_lowercase=True)\n",
    "\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert start-end pos to token start/end pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "        # if None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
