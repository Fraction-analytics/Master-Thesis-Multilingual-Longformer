#+TITLE: Large-Context Question Answering with Cross-Lingual Transfer
#+OPTIONS: H:4 toc:2
#+STARTUP: inlineimages
# +OPTIONS num:t

* Table of Contents :TOC:
- [[#description][Description]]
- [[#motivation][Motivation]]
- [[#frameworks-used][Frameworks Used]]
- [[#usage-and-setup][Usage and Setup]]
- [[#transfer-long-context-to-language-models][Transfer Long-Context to Language Models]]
  - [[#pre-requisites][Pre-Requisites]]
  - [[#roberta][RoBERTa]]
  - [[#xlm-r][XLM-R]]
- [[#todo-fine-tune-on-question-answering-tasks][~TODO~ Fine-Tune on Question Answering Tasks]]
  - [[#todo-squad-11][~TODO~ SQuAD 1.1]]
  - [[#todo-trivia-qa][~TODO~ Trivia QA]]
  - [[#todo-xquad][~TODO~ XQuAD]]
  - [[#todo-tydi-qa][~TODO~ TyDi QA]]
- [[#todo][TODO]]
- [[#meta][Meta]]

* Description
Master thesis work for investigating the transferability of long-term context on transformer models, especially in terms for multilingual models. The script provided includes the necessary steps to reproduce the result presented in the master thesis. We convert pre-trained monolingual and multilingual language models on English to Longformer models to a maximum model length of 4096.

1. A Monolingual model are compare to a RoBERTa-base and Longformer-base-4096  on SQuAD 1.1 and TriviaQA.
2. A Multilingual model are compared to an XLM-R-base model on XQuAD and TyDi QA

The model are trained in docker containers for reproducability.
* TODO Motivation

* Frameworks Used
*Built with*
+ [[https://github.com/huggingface/transformers][Transformers]]
+ [[https://github.com/huggingface/datasets][Datasets]]
* Usage and Setup
Makefiles are includes for quick setup, execution and tear-down
*Setup Dockerfile and and Requirements*
#+begin_src shell
make build && make up
#+end_src

*Run an Example File*
#+begin_src shell
make repl run="scripts/test_logging.py --output_file /workspace/logs/test.log"
#+end_src

*Tear-Down*
#+begin_src shell
make down
#+end_src

* Transfer Long-Context to Language Models
Converting transformer models are based on the [[https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb][Longformer conversion script]]. The script can be run for any pre-trained RoBERTa based model and can be extended to be used with other pre-traned models.

Training with these parameters on a ~TODO~ 64 GPU takes ~5 days
** Pre-Requisites
Download the WikiText-103 and unpack unpack to a data directory
~TODO~

** RoBERTa
# RoBERTa Longformer training
Docker container for training longformer from pretrained model s.a. RoBERTa model. The script `scripts/longformer.py' trains a pretrained language model to a given max sequence length. The script `scripts/run_squad.py' fine-tunes on the SQuAD 1.1 QA dataset.

#+begin_src shell
export SEED=42
export MAX_POS=4096
export MODEL_NAME_OR_PATH=roberta-base
export MODEL_NAME=$MODEL_NAME_OR_PATH-$MAX_POS-seed-$SEED-fast-lm
export MODEL_DIR=/workspace/models
export DATA_DIR=/workspace/data/wikitext-103-raw
export LOG_DIR=/workspace/logs

make repl run="scripts/run_long_lm.py \
    --model_name_or_path $MODEL_NAME_OR_PATH \
    --model_name $MODEL_NAME \
    --output_dir $MODEL_DIR/$MODEL_NAME \
    --logging_dir $LOG_DIR/$MODEL_NAME \
    --val_file_path $DATA_DIR/wiki.valid.raw \
    --train_file_path $DATA_DIR/wiki.train.raw \
    --seed $SEED \
    --max_pos $MAX_POS \
    --adam_epsilon 1e-8 \
    --warmup_steps 500 \
    --learning_rate 3e-5 \
    --weight_decay 0.01 \
    --max_steps 6000 \
    --evaluate_during_training \
    --logging_steps 50 \
    --eval_steps 50 \
    --save_steps 500  \
    --max_grad_norm 1.0 \
    --per_device_eval_batch_size 2 \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 64 \
    --overwrite_output_dir \
    --fp16 \
    --do_train \
    --do_eval
"

#+end_src
** XLM-R
#+begin_src shell
export SEED=42
export MAX_POS=4096
export MODEL_NAME_OR_PATH=xlm-roberta-base
export MODEL_NAME=$MODEL_NAME_OR_PATH-$MAX_POS-seed-$SEED-fast-lm
export MODEL_DIR=/workspace/models
export DATA_DIR=/workspace/data/wikitext-103-raw
export LOG_DIR=/workspace/logs

make repl run="scripts/run_long_lm.py \
    --model_name_or_path $MODEL_NAME_OR_PATH \
    --model_name $MODEL_NAME \
    --output_dir $MODEL_DIR/$MODEL_NAME \
    --logging_dir $LOG_DIR/$MODEL_NAME \
    --val_file_path $DATA_DIR/wiki.valid.raw \
    --train_file_path $DATA_DIR/wiki.train.raw \
    --seed $SEED \
    --max_pos $MAX_POS \
    --adam_epsilon 1e-8 \
    --warmup_steps 500 \
    --learning_rate 3e-5 \
    --weight_decay 0.01 \
    --max_steps 6000 \
    --evaluate_during_training \
    --logging_steps 50 \
    --eval_steps 50 \
    --save_steps 500  \
    --max_grad_norm 1.0 \
    --per_device_eval_batch_size 2 \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 64 \
    --overwrite_output_dir \
    --fp16 \
    --do_train \
    --do_eval
"

#+end_src

* ~TODO~ Fine-Tune on Question Answering Tasks
** ~TODO~ SQuAD 1.1

#+begin_src shell
export MODEL_DIR=/workspace/models/RoBERTa_Long_seed_1337/RoBERTa_Long_seed_1337-4096-lm
export LOG_DIR=/workspace/logs
export SQUAD_DIR=/workspace/data/SQuAD
export SEED=42

make repl run="scripts/run_squad.py \
    --model_type roberta \
    --model_name_or_path $MODEL_DIR \
    --tokenizer_name roberta-base \
    --do_train \
    --do_eval \
    --fp16 \
    --seed $SEED \
    --do_lower_case \
    --train_file $SQUAD_DIR/train-v1.1.json \
    --predict_file $SQUAD_DIR/dev-v1.1.json \
    --per_gpu_train_batch_size 12 \
    --learning_rate 3e-5 \
    --num_train_epochs 3.0 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir /workspace/models/debug_squad
"

#+end_src

** ~TODO~ Trivia QA
** ~TODO~ XQuAD
** ~TODO~ TyDi QA


* TODO
+ Download Wikitext 103
+ Run the pre-training and fine-tuning and which datasets
+ Table of the evaluation matrix
+ BigBird and Future Works
+ Link to Report
+ Cite
+ Link to presentation
+ Describe runs made with multiple different seeds
+ Provide the notebook for plotting the results?
+ Give Guide to what to change in the .env-file

*** TODO Acknowledgment
Many thanks to the [[https://github.com/allenai/longformer][Longformer Authors]] for providing reproducible training scripts and Huggingface for open-sourcing their models and frameworks. I would like to thank my supervisor at Peltarion Philipp Eisen for his invaluable feedback, insight and availability. Thank you Professor Joakim Nivre for insightful and thorough feedback and for taking the time out of your busy schedule. A massive thank you to all the wonderful people at Peltarion for the opportunity to work on such an interesting project.

*** TODO Citation
If you found this repo or the thesis results useful, please cite the thesis:
#+begin_src text
@misc{Bergkvist1436450,
   author = {Bergkvist, Alexander and Hedberg, Nils and Rollino, Sebastian and Sagen, Markus},
   institution = {Uppsala University, Department of Information Technology},
   pages = {56},
   school = {Uppsala University, Department of Information Technology},
   title = {Surmize: An Online NLP System for Close-Domain Question-Answering and Summarization},
   series = {Självständigt arbete i informationsteknologi},
   number = {2020-001},
   year = {2020}
}
#+end_src

* TODO Meta
Markus Sagen - [[mailto:markus.john.sagen@gmail.com][markus.john.sagen@gmail.com]]
Distributed under XYZ license
